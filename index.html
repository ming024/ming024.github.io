<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Chung-Ming Chien (簡仲明)</title> <meta name="author" content="Chung-Ming Chien (簡仲明)"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta property="og:site_name" content="Chung-Ming Chien (簡仲明)"> <meta property="og:type" content="website"> <meta property="og:title" content="Chung-Ming Chien (簡仲明) | about"> <meta property="og:url" content="https://ming024.github.io/"> <meta property="og:description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta property="og:image" content="20220731_chiaminglake.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="about"> <meta name="twitter:description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="twitter:image" content="20220731_chiaminglake.png"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <script src="https://kit.fontawesome.com/a076d05399.js" crossorigin="anonymous"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/cm.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ming024.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/experience/">experience</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/Chung_Ming_Chien_CV_20240923.pdf">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Chung-Ming Chien</span> (簡仲明) </h1> <p class="desc">Chicago, Illinois, United States</p> </header> <article> <div class="profile float-right"> <figure> <picture> <img src="/assets/img/20230530_santorini.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="20230530_santorini.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <center><p>Santorini, Greece</p></center> <center><p>May 30, 2023</p></center> </div> </div> <div class="clearfix"> <p>I am a 3rd-year Ph.D. student at <a href="https://www.ttic.edu" rel="external nofollow noopener" target="_blank">Toyota Technological Institute at Chicago (TTIC)</a>, where I am fortunate to work with <a href="https://home.ttic.edu/~klivescu/" rel="external nofollow noopener" target="_blank">Karen Livescu</a>. My research interests encompass the fields of speech and natural language processing technologies. Here are some topics I have been focusing on recently:</p> <ul> <li> <strong>Speech Language Models</strong><br> How to facilitate speech applications with the aids of the knowledge learned from text by pre-trained large language models (LLMs)? How to fine-tune LLMs to take speech inputs and outputs and enable general-purpose speech conversational AI?</li> <li> <strong>Speech Generation</strong><br> Control and model non-lexical information in generated speech in a more efficient and intuitive way.</li> <li> <strong>Self-Supervised Speech Representations</strong><br> Analyze the information encoded in self-supervised speech representations and explore various applications for the learned representations and units.</li> </ul> <p>Prior to joining TTIC, I earned my Master’s degree in Computer Science from <a href="https://www.csie.ntu.edu.tw/main.php" rel="external nofollow noopener" target="_blank">National Taiwan University (NTU)</a>, where I had the privilege of working with <a href="https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm" rel="external nofollow noopener" target="_blank">Lin-shan Lee</a> and <a href="https://speech.ee.ntu.edu.tw/~hylee/index.php" rel="external nofollow noopener" target="_blank">Hung-yi Lee</a> at the <a href="https://speech.ee.ntu.edu.tw" rel="external nofollow noopener" target="_blank">Speech Processing Lab</a>. Outside of school, I also gained valuable experience through summer internships with <a href="https://www.amazon.science/tag/text-to-speech" rel="external nofollow noopener" target="_blank">Amazon Alexa TTS Research</a>, <a href="https://ai.meta.com" rel="external nofollow noopener" target="_blank">FAIR (AI at Meta)</a>, and <a href="https://research.nvidia.com/labs/conv-ai/" rel="external nofollow noopener" target="_blank">NVIDIA</a>.</p> <p>Beyond my academic pursuits, I am a sports enthusiast and amateur athlete. I captained the baseball varsity team of NTU during my undergraduate years. I am also broadly interested in tennis, hiking, scuba diving, swimming, badminton, and training. In 2022, I achieved a personal milestone by completing my first marathon, and I have been dedicated to improving my PB with the goal of breaking the 3:10 mark!</p> </div> <div class="news"> <h2>news</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jun 4, 2024</th> <td> <a href="https://arxiv.org/abs/2406.06251" rel="external nofollow noopener" target="_blank">“Learning Fine‑Grained Controllability on Speech Generation via Efficient Fine‑Tuning”</a> is accepted to InterSpeech 2024! </td> </tr> <tr> <th scope="row">May 16, 2024</th> <td> <a href="https://arxiv.org/abs/2406.10083" rel="external nofollow noopener" target="_blank">“On the Evaluation of Speech Foundation Models for Spoken Language Understanding”</a> is accepted to Findings of ACL 2024! </td> </tr> <tr> <th scope="row">Apr 16, 2024</th> <td> I gave a talk at Midwest Speech and Language Days in Ann Arbor, Michigan <img class="emoji" title=":microphone:" alt=":microphone:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a4.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Apr 9, 2024</th> <td> I successfully passed the qualifying exam of TTIC and will soon become a Ph.D. candidate <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jan 23, 2024</th> <td> <a href="https://arxiv.org/abs/2307.00162" rel="external nofollow noopener" target="_blank">“What Do Self‑Supervised Speech Models Know about Words”</a> is accepted to TACL 2024! </td> </tr> <tr> <th scope="row">Jan 18, 2024</th> <td> I will join <a href="https://research.nvidia.com/labs/conv-ai/" rel="external nofollow noopener" target="_blank">NVIDIA NeMo Team</a> for my 2024 summer internship and will work on speech language models! </td> </tr> <tr> <th scope="row">Jan 13, 2024</th> <td> My open-source <a href="https://github.com/ming024/FastSpeech2" rel="external nofollow noopener" target="_blank">FastSpeech 2</a> project gets over 1.5k stars on Github <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Dec 20, 2023</th> <td> I share the honor of the Best Student Paper Award of ASRU 2023 with Mingjiamei, Ju-Chieh, and Karen. Check out our work <a href="https://arxiv.org/abs/2310.05919" rel="external nofollow noopener" target="_blank">“Few-shot SLU via Joint Speech-Text Models”</a> for more details <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Oct 7, 2023</th> <td> <a href="https://arxiv.org/abs/2310.08715" rel="external nofollow noopener" target="_blank">“Toward Joint Language Modeling for Speech Units and Text”</a> is accepted to Findings of EMNLP 2023! </td> </tr> <tr> <th scope="row">Sep 22, 2023</th> <td> Our work <a href="https://arxiv.org/abs/2310.05919" rel="external nofollow noopener" target="_blank">“Few-shot SLU via Joint Speech-Text Models”</a> is accepted at ASRU 2023, and I’ll surely go back Taiwan to present it in person! </td> </tr> </table> </div> </div> <div class="publications"> <h2>selected publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#B509AC"><a href="https://www.isca-speech.org/archive/interspeech_2024/index.html" rel="external nofollow noopener" target="_blank">InterSpeech 2024</a></abbr></div> <div id="chien2024learning" class="col-sm-8"> <div class="title">Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning</div> <div class="author"> <em>Chung-Ming Chien</em>, Andros Tjandra, Apoorv Vyas, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Matt Le, Bowen Shi, Wei-Ning Hsu' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> In <em>Interspeech 2024</em> </div> <div class="links"> <a href="http://arxiv.org/abs/2406.06251" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/poster_chien2024learning.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chien2024learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chien, Chung-Ming and Tjandra, Andros and Vyas, Apoorv and Le, Matt and Shi, Bowen and Hsu, Wei-Ning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Interspeech 2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2406.06251}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{eess.AS}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#A6A1F3"><a href="https://ieeexplore.ieee.org/xpl/conhome/10388490/proceeding" rel="external nofollow noopener" target="_blank">ASRU 2023</a></abbr></div> <div id="chien2023few" class="col-sm-8"> <div class="title">Few-Shot Spoken Language Understanding via Joint Speech-Text Models</div> <div class="author"> <em>Chung-Ming Chien</em>, Mingjiamei Zhang, Ju-Chieh Chou, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Karen Livescu' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="info"> <small><b>Best Student Paper Award</b></small> </div> <div class="periodical"> In <em>2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em> </div> <div class="links"> <a href="http://arxiv.org/abs/2310.05919" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/poster_chien2023few.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/slides_chien2023few.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://www.youtube.com/watch?v=8K8BcCt8UjI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chien2023few</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Few-Shot Spoken Language Understanding via Joint Speech-Text Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chien, Chung-Ming and Zhang, Mingjiamei and Chou, Ju-Chieh and Livescu, Karen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2310.05919}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://ieeexplore.ieee.org/xpl/conhome/9413349/proceeding" rel="external nofollow noopener" target="_blank">ICASSP 2021</a></abbr></div> <div id="chien2020fragmentvc" class="col-sm-8"> <div class="title">FragmentVC: Any-To-Any Voice Conversion by End-To-End Extracting and Fusing Fine-Grained Voice Fragments with Attention</div> <div class="author"> <em>Chung-Ming Chien*</em>, Yist Y. Lin*, Jheng-Hao Lin, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Hung-yi Lee, Lin-shan Lee' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="info"> <small><b>*equal contribution</b></small> </div> <div class="periodical"> In <em>ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> </div> <div class="links"> <a href="http://arxiv.org/abs/2010.14150" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/yistLin/FragmentVC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/poster_chien2020fragmentvc.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/slides_chien2020fragmentvc.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chien2020fragmentvc</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chien*, Chung-Ming and Lin*, Yist Y. and Lin, Jheng-Hao and Lee, Hung-yi and Lee, Lin-shan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FragmentVC: Any-To-Any Voice Conversion by End-To-End Extracting and Fusing Fine-Grained Voice Fragments with Attention}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5939-5943}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICASSP39728.2021.9413699}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2010.14150}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{eess.AS}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%63%6D%63%68%69%65%6E@%74%74%69%63.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=R3Wh6vUAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/ming024" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/chungmingchien" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://open.spotify.com/user/11101814651" title="Last FM" rel="external nofollow noopener" target="_blank"><i class="fab fa-spotify"></i></a> </div> <div class="contact-note"> Please reach me at my e-mail if you have any question or are interested in any form of collaboration! </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Chung-Ming Chien (簡仲明). Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: September 23, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-20JYE3VHJ0"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-20JYE3VHJ0");</script> <script>!function(e,a,t,i,s){e[s]=e[s]||function(){(e[s].v=e[s].v||[]).push(arguments)},e._visaSettings||(e._visaSettings={}),e._visaSettings[i]={v:"1.0",s:i,a:"1",t:s};var n=a.getElementsByTagName("body")[0],c=a.createElement("script");c.defer=1,c.async=1,c.src=t+"?s="+i,n.appendChild(c)}(window,document,"//app-worker.visitor-analytics.io/main.js","dac2ed68-b929-11ed-b589-901b0edac50a","va");</script> <script>!function(e,a,t,i,s){e[s]=e[s]||function(){(e[s].v=e[s].v||[]).push(arguments)},e._visaSettings||(e._visaSettings={}),e._visaSettings[i]={v:"1.0",s:i,a:"1",t:s};var n=a.getElementsByTagName("body")[0],c=a.createElement("script");c.defer=1,c.async=1,c.src=t+"?s="+i,n.appendChild(c)}(window,document,"//app-worker.visitor-analytics.io/main.js","10f40097-1bf5-11ec-b589-901b0edac50a","va");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>